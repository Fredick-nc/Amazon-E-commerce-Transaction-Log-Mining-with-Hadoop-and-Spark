{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark-submit project3.py \"file:///home/sample.csv\" \"file:///home/output\" 0.5\n",
    "# spark-submit project3.py \"file:///home/testcase1.csv\" \"file:///home/output\" 0.7\n",
    "# spark-submit project3.py \"file:///home/testcase2.csv\" \"file:///home/output\" 0.8\n",
    "# time spark-submit project3.py \"file:///home/testcase1.csv\" \"file:///home/output\" 0.7\n",
    "# time spark-submit project3.py \"file:///home/testcase2.csv\" \"file:///home/output\" 0.8\n",
    "# testcase 1\n",
    "# real0m11.479s\n",
    "# user0m16.043s\n",
    "# sys0m0.623s\n",
    "# testcase 2\n",
    "# real0m28.500s\n",
    "# user0m27.747s\n",
    "# sys0m0.688s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52fd299",
   "metadata": {},
   "source": [
    "背景：相似性连接是数据挖掘中的一项重要任务。 给定一组记录，其目标是找到相似度大于阈值的所有记录对。 记录是一组项目。 例如，在亚马逊中，一条记录可以代表客户在单次交易中购买的一组产品。 该问题可以在不同领域找到各种应用，例如数据清洗和产品推荐。\n",
    "\n",
    "给定记录集合 R、相似度函数 sim() 和阈值 t，R 上的相似度连接就是从 R 中找到所有记录对 r 和 s，使得 sim(r, s)>=t。 在这个项目中，我们使用 Jaccard 相似度函数来计算相似度\n",
    "\n",
    "问题定义：在这个项目中，我们仍然将使用客户购买交易日志的电子商务数据集。 数据集中的每条记录都有以下五个字段（请参阅示例数据集）：\n",
    "\n",
    "InvoiceNo: the unique ID to record one purchase transaction\n",
    "\n",
    "Description: the name of the item in a transaction (a name can contain multiple characters)\n",
    "\n",
    "Quantity: the amount of the items purchased\n",
    "\n",
    "InvoiceDate: the time of the transaction\n",
    "\n",
    "UnitPrice: the price of a single item\n",
    "\n",
    "每笔交易都包含一组购买的物品。 例如，在示例数据集中，交易 1 = {A, B, C}，交易 2 = {A, C, DD} 和交易 3 = {A, B, C, DD}。 您的任务是利用 Spark 查找不同年份的所有相似交易对。\n",
    "\n",
    "输出格式：输出文件包含所有相似交易及其相似之处。 输出格式为“(发票号1,发票号2):相似度值”。 在每对中，交易必须来自不同年份且 InvoiceNo1 < InvoiceNo2。 输出结果不应有重复。 这些对按升序排序（按第一个，然后是第二个）。 给定上述阈值为 0.5 的示例数据集，输出结果应为：\n",
    "(1,3):0.75\n",
    "(2,3):0.75\n",
    "(1,2)：不返回 0.5，因为交易 1 和 2 是同一年."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_month_year(date):\n",
    "    date = date.split(' ')[0]\n",
    "    split_date = date.split(\"/\")\n",
    "    new_date = \"/\".join([split_date[1], split_date[2]])\n",
    "    return new_date\n",
    "\n",
    "class project3:\n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        conf = SparkConf()\n",
    "        sc = SparkContext(conf = conf)\n",
    "\n",
    "        data= sc.textFile(inputpath)\n",
    "        data = data.map(lambda line: line.split(','))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #final_result.coalesce(1).saveAsTextFile(outputPath)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f21e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import combinations\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "class project3:           \n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        spark = SparkSession.builder.master(\"local\").appName(\"project3_df\").getOrCreate()\n",
    "        \n",
    "        # Read the input CSV file and give the column names for them\n",
    "        data = spark.read.csv(inputpath).toDF(\"InvoiceNo\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\")\n",
    "        \n",
    "        # Select needed columns\n",
    "        data = data.select(\"InvoiceNo\", \"Description\",\"InvoiceDate\")\n",
    "        \n",
    "        # Extract month and year from the InvoiceDate field \n",
    "        data_filtered = data.withColumn(\"Year\",  split(split(data[\"InvoiceDate\"], ' ')[0], '/')[2])\n",
    "        data_filtered = data_filtered.drop('InvoiceDate')\n",
    "\n",
    "\n",
    "        \n",
    "        grouped_data = data_filtered.groupBy(\"InvoiceNo\", \"Year\").agg(concat_ws(\" \", collect_list(\"Description\")).alias(\"Description\"))\n",
    "        data_with_value = grouped_data.withColumn(\"InvoiceNo_Items\", concat_ws(\",\", col(\"InvoiceNo\"), col(\"Description\")))\n",
    "\n",
    "        # Explode the Description column to create a new row for each element\n",
    "        exploded_data = data_with_value.withColumn(\"Key\", explode(split(\"Description\", \" \")))\n",
    "\n",
    "        data_partition_by_prefix = exploded_data.select(\"Key\", \"InvoiceNo_Items\", \"Year\")\n",
    "\n",
    "        def jaccard_similarity(set1, set2):\n",
    "            intersection_size = len(set(set1).intersection(set2))\n",
    "            union_size = len(set(set1).union(set2))\n",
    "            return intersection_size / union_size if union_size != 0 else 0\n",
    "\n",
    "        self_joined_data = data_partition_by_prefix.alias(\"df1\").join(data_partition_by_prefix.alias(\"df2\"), \"Key\")\n",
    "        \n",
    "        \n",
    "        renamed_data = self_joined_data.select(col(\"df1.Key\").alias(\"Key\"),\n",
    "                     col(\"df1.InvoiceNo_Items\").alias(\"InvoiceNo_Items_1\"),\n",
    "                     col(\"df1.Year\").alias(\"Year_1\"),\n",
    "                     col(\"df2.InvoiceNo_Items\").alias(\"InvoiceNo_Items_2\"),\n",
    "                     col(\"df2.Year\").alias(\"Year_2\"),\n",
    "                     regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "                     regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "                )\n",
    "        \n",
    "        splitted_data = renamed_data.select(\"Key\", \n",
    "                     substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "                     \"Year_1\",\n",
    "                     substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "                     \"Year_2\",\n",
    "                     \"InvoiceNo_1\",\n",
    "                     \"InvoiceNo_2\"\n",
    "                )\n",
    "\n",
    "        paired_data = splitted_data.withColumn(\"pair\", concat(lit(\"(\"), col(\"InvoiceNo_1\"), lit(\",\"), col(\"InvoiceNo_2\"), lit(\")\")))\n",
    "        \n",
    "        filtered_data = paired_data.filter(col(\"Year_1\") != col(\"Year_2\"))\n",
    "        \n",
    "        filtered_data.show()\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fca7b1a5",
   "metadata": {},
   "source": [
    "+---------+----+-----------+\n",
    "|InvoiceNo|Year|Description|\n",
    "+---------+----+-----------+\n",
    "|        2|2010|          A|\n",
    "|        2|2010|          C|\n",
    "|        2|2010|         DD|\n",
    "|        1|2010|          A|\n",
    "|        1|2010|          B|\n",
    "|        1|2010|          C|\n",
    "|        3|2011|          A|\n",
    "|        3|2011|          B|\n",
    "|        3|2011|          C|\n",
    "|        3|2011|         DD|\n",
    "+---------+----+-----------+\n",
    "\n",
    "\n",
    "+---------+----+-----------+\n",
    "|InvoiceNo|Year|Description|\n",
    "+---------+----+-----------+\n",
    "|        2|2010|     A C DD|\n",
    "|        1|2010|      A B C|\n",
    "|        3|2011|   A B C DD|\n",
    "+---------+----+-----------+\n",
    "\n",
    "\n",
    "\n",
    "+---+---------------+----+\n",
    "|Key|InvoiceNo_Items|Year|\n",
    "+---+---------------+----+\n",
    "|  A|       2,A C DD|2010|\n",
    "|  C|       2,A C DD|2010|\n",
    "| DD|       2,A C DD|2010|\n",
    "|  A|        1,A B C|2010|\n",
    "|  B|        1,A B C|2010|\n",
    "|  C|        1,A B C|2010|\n",
    "|  A|     3,A B C DD|2011|\n",
    "|  B|     3,A B C DD|2011|\n",
    "|  C|     3,A B C DD|2011|\n",
    "| DD|     3,A B C DD|2011|\n",
    "+---+---------------+----+\n",
    "\n",
    "\n",
    "+---+---------------+----+\n",
    "|Key|InvoiceNo_Items|Year|\n",
    "+---+---------------+----+\n",
    "|  A|       2,A C DD|2010|\n",
    "|  A|        1,A B C|2010|\n",
    "|  A|     3,A B C DD|2011|\n",
    "|  B|        1,A B C|2010|\n",
    "|  B|     3,A B C DD|2011|\n",
    "|  C|       2,A C DD|2010|\n",
    "|  C|        1,A B C|2010|\n",
    "|  C|     3,A B C DD|2011|\n",
    "| DD|       2,A C DD|2010|\n",
    "| DD|     3,A B C DD|2011|\n",
    "+---+---------------+----+\n",
    "\n",
    "\n",
    "\n",
    "+---+-----------------+------+-----------------+------+-----------+-----------+\n",
    "|Key|InvoiceNo_Items_1|Year_1|InvoiceNo_Items_2|Year_2|InvoiceNo_1|InvoiceNo_2|\n",
    "+---+-----------------+------+-----------------+------+-----------+-----------+\n",
    "|  A|         2,A C DD|  2010|       3,A B C DD|  2011|          2|          3|\n",
    "|  A|         2,A C DD|  2010|          1,A B C|  2010|          2|          1|\n",
    "|  A|         2,A C DD|  2010|         2,A C DD|  2010|          2|          2|\n",
    "|  C|         2,A C DD|  2010|       3,A B C DD|  2011|          2|          3|\n",
    "|  C|         2,A C DD|  2010|          1,A B C|  2010|          2|          1|\n",
    "|  C|         2,A C DD|  2010|         2,A C DD|  2010|          2|          2|\n",
    "| DD|         2,A C DD|  2010|       3,A B C DD|  2011|          2|          3|\n",
    "| DD|         2,A C DD|  2010|         2,A C DD|  2010|          2|          2|\n",
    "|  A|          1,A B C|  2010|       3,A B C DD|  2011|          1|          3|\n",
    "|  A|          1,A B C|  2010|          1,A B C|  2010|          1|          1|\n",
    "|  A|          1,A B C|  2010|         2,A C DD|  2010|          1|          2|\n",
    "|  B|          1,A B C|  2010|       3,A B C DD|  2011|          1|          3|\n",
    "|  B|          1,A B C|  2010|          1,A B C|  2010|          1|          1|\n",
    "|  C|          1,A B C|  2010|       3,A B C DD|  2011|          1|          3|\n",
    "|  C|          1,A B C|  2010|          1,A B C|  2010|          1|          1|\n",
    "|  C|          1,A B C|  2010|         2,A C DD|  2010|          1|          2|\n",
    "|  A|       3,A B C DD|  2011|       3,A B C DD|  2011|          3|          3|\n",
    "|  A|       3,A B C DD|  2011|          1,A B C|  2010|          3|          1|\n",
    "|  A|       3,A B C DD|  2011|         2,A C DD|  2010|          3|          2|\n",
    "|  B|       3,A B C DD|  2011|       3,A B C DD|  2011|          3|          3|\n",
    "+---+-----------------+------+-----------------+------+-----------+-----------+\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "+---+-----------------+------+-----------------+------+-----+\n",
    "|Key|InvoiceNo_Items_1|Year_1|InvoiceNo_Items_2|Year_2| pair|\n",
    "+---+-----------------+------+-----------------+------+-----+\n",
    "|  A|           A C DD|  2010|         A B C DD|  2011|(2,3)|\n",
    "|  C|           A C DD|  2010|         A B C DD|  2011|(2,3)|\n",
    "| DD|           A C DD|  2010|         A B C DD|  2011|(2,3)|\n",
    "|  A|            A B C|  2010|         A B C DD|  2011|(1,3)|\n",
    "|  B|            A B C|  2010|         A B C DD|  2011|(1,3)|\n",
    "|  C|            A B C|  2010|         A B C DD|  2011|(1,3)|\n",
    "|  A|         A B C DD|  2011|            A B C|  2010|(3,1)|\n",
    "|  A|         A B C DD|  2011|           A C DD|  2010|(3,2)|\n",
    "|  B|         A B C DD|  2011|            A B C|  2010|(3,1)|\n",
    "|  C|         A B C DD|  2011|            A B C|  2010|(3,1)|\n",
    "|  C|         A B C DD|  2011|           A C DD|  2010|(3,2)|\n",
    "| DD|         A B C DD|  2011|           A C DD|  2010|(3,2)|\n",
    "+---+-----------------+------+-----------------+------+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8169f11",
   "metadata": {},
   "source": [
    "# Efficiency/Similarity Two-step Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c828bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "\n",
    "class project3:           \n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        spark = SparkSession.builder.master(\"local\").appName(\"project3_df\").getOrCreate()\n",
    "        \n",
    "        # Read the input CSV file and give the column names for them\n",
    "        data = spark.read.csv(inputpath).toDF(\"InvoiceNo\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\")\n",
    "        \n",
    "        # Select needed columns\n",
    "        data = ( \n",
    "             data\n",
    "            .select(\"InvoiceNo\", \"Description\",\"InvoiceDate\")\n",
    "            .withColumn(\"Year\",  split(split(data[\"InvoiceDate\"], ' ')[0], '/')[2])\n",
    "            .drop('InvoiceDate')\n",
    "        )\n",
    "        \n",
    "        grouped_data = (\n",
    "             data.groupBy(\"InvoiceNo\", \"Year\")\n",
    "             .agg(concat_ws(\" \", collect_list(\"Description\")).alias(\"Description\"))\n",
    "        )\n",
    "            \n",
    "        data_partition_by_prefix = (\n",
    "             grouped_data\n",
    "             .withColumn(\"InvoiceNo_Items\", concat_ws(\",\", col(\"InvoiceNo\"), col(\"Description\")))\n",
    "             .withColumn(\"Key\", explode(split(\"Description\", \" \")))\n",
    "             .select(\"Key\", \"InvoiceNo_Items\", \"Year\")\n",
    "        )\n",
    "\n",
    "\n",
    "        def jaccard_similarity(s1, s2):\n",
    "            set1 = set(s1.split())\n",
    "            set2 = set(s2.split())\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return (intersection) / union\n",
    "\n",
    "        self_joined_data = (\n",
    "            data_partition_by_prefix\n",
    "            .alias(\"df1\")\n",
    "            .join(data_partition_by_prefix\n",
    "            .alias(\"df2\"), \"Key\")\n",
    "        )\n",
    "        \n",
    "        \n",
    "        renamed_data = (\n",
    "            self_joined_data\n",
    "            .select(col(\"df1.Key\").alias(\"Key\"),\n",
    "                     col(\"df1.InvoiceNo_Items\").alias(\"InvoiceNo_Items_1\"),\n",
    "                     col(\"df1.Year\").alias(\"Year_1\"),\n",
    "                     col(\"df2.InvoiceNo_Items\").alias(\"InvoiceNo_Items_2\"),\n",
    "                     col(\"df2.Year\").alias(\"Year_2\"),\n",
    "                     regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "                     regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        splitted_data = (\n",
    "            renamed_data\n",
    "            .select(\"Key\", \n",
    "                     substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "                     \"Year_1\",\n",
    "                     substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "                     \"Year_2\",\n",
    "                     \"InvoiceNo_1\",\n",
    "                     \"InvoiceNo_2\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "        sort_tuple_udf = udf(lambda pair: str(tuple(sorted(eval(pair)))), StringType())\n",
    "\n",
    "        filtered_data = (\n",
    "            splitted_data\n",
    "            .withColumn(\"pair\", \n",
    "                        concat(lit(\"(\"), col(\"InvoiceNo_1\"), lit(\",\"), col(\"InvoiceNo_2\"), lit(\")\"))\n",
    "            )\n",
    "            .filter(col(\"Year_1\") != col(\"Year_2\")).drop(\"InvoiceNo_1\",\"InvoiceNo_2\")\n",
    "            .withColumn(\"sorted_pair\", sort_tuple_udf(\"pair\"))\n",
    "            .drop(\"pair\")\n",
    "        )\n",
    "        \n",
    "        result_df = (\n",
    "            filtered_data\n",
    "            .withColumn(\"Similarity\", \n",
    "                        jaccard_similarity_udf(col(\"InvoiceNo_Items_1\"), col(\"InvoiceNo_Items_2\"))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result_df = (\n",
    "            result_df\n",
    "            .select('sorted_pair','Similarity')\n",
    "            .dropDuplicates([\"sorted_pair\", \"Similarity\"])\n",
    "            .orderBy('sorted_pair')\n",
    "            .filter(col(\"Similarity\") >= k)\n",
    "        )\n",
    "        \n",
    "        result_format_df = (\n",
    "            result_df\n",
    "            .select(concat_ws(\":\", col(\"sorted_pair\"), col(\"Similarity\")).alias('result'))\n",
    "            .orderBy(regexp_extract(\"result\", r\"\\((\\d+),\", 1).cast(\"int\"))\n",
    "        )\n",
    "        result_format_df = result_format_df.withColumn(\"result\", regexp_replace(\"result\", \",\\\\s*\", \",\"))\n",
    "        #sorted_result_df.show()\n",
    "        \n",
    "        result_format_df.write.format(\"csv\").option(\"header\", \"false\").save(outputpath)\n",
    "        \n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41d15e",
   "metadata": {},
   "source": [
    "# Efficiency/Similarity Exact Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class project3:           \n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        spark = SparkSession.builder.master(\"local\").appName(\"project3_df\").getOrCreate()\n",
    "        \n",
    "        # Read the input CSV file and give the column names for them\n",
    "        data = spark.read.csv(inputpath).toDF(\"InvoiceNo\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\")\n",
    "        \n",
    "        # Select needed columns\n",
    "        data = data.select(\"InvoiceNo\", \"Description\",\"InvoiceDate\")\n",
    "        \n",
    "        # Extract month and year from the InvoiceDate field \n",
    "        data_filtered = data.withColumn(\"Year\",  split(split(data[\"InvoiceDate\"], ' ')[0], '/')[2])\n",
    "        data_filtered = data_filtered.drop('InvoiceDate')\n",
    "        \n",
    "        grouped_data = data_filtered.groupBy(\"InvoiceNo\", \"Year\").agg(concat_ws(\" \", collect_list(\"Description\")).alias(\"Description\"))\n",
    "        data_with_value = grouped_data.withColumn(\"InvoiceNo_Items\", concat_ws(\",\", col(\"InvoiceNo\"), col(\"Description\")))\n",
    "\n",
    "        cal_token_frequencies = data_with_value.select('InvoiceNo','Description').withColumn(\"tokens\", split(\"Description\", \" \"))\n",
    "        df_exploded = cal_token_frequencies.select(\"InvoiceNo\", explode(\"tokens\").alias(\"token\"))\n",
    "        token_frequencies = df_exploded.groupBy(\"token\").agg(count(\"token\").alias(\"frequency\")).orderBy('frequency')\n",
    "        tokens = token_frequencies.select(\"token\")\n",
    "       \n",
    "        data_original = data_with_value.select('InvoiceNo','Description','InvoiceNo_Items','Year').orderBy('InvoiceNo')\n",
    "        # Calculate prefixes for each record\n",
    "        #windowSpec = Window.partitionBy(\"tokens\")\n",
    "        #df_swapped = df_swapped.withColumn(\"prefix\", concat_ws(\",\", collect_list(\"token\").over(window_spec)))\n",
    "        \n",
    "        #token_frequencies.show()\n",
    "        #result_df = data_original.groupBy(tokens).agg(collect_list(\"InvoiceNo\"), collect_list(\"Description\"))\n",
    "        #sorted_result_df.write.format(\"csv\").option(\"header\", \"false\").save(outputpath)\n",
    "        token_list = [row.token for row in tokens.collect()]\n",
    "        # def find_prefix(description):\n",
    "        #     prefixes = []\n",
    "        #     for token in token_list:\n",
    "        #         if token in description:\n",
    "        #             prefixes.append(token)\n",
    "        #     return prefixes\n",
    "        \n",
    "        def find_prefix(description):\n",
    "            prefixes = []\n",
    "            for token in token_list:\n",
    "                if token in description:\n",
    "                    prefixes.append(token)\n",
    "            prefixes.pop()\n",
    "            return prefixes\n",
    "\n",
    "        find_prefix_udf = udf(find_prefix, ArrayType(StringType()))\n",
    "        #data_with_prefix = data_original.withColumn(\"Prefixes\", find_prefix_udf(col(\"Description\")))\n",
    "        \n",
    "        data_with_remaining_tokens = data_original.withColumn(\"Tokens\", find_prefix_udf(col(\"Description\")))\n",
    "        \n",
    "        exploded_data = data_with_remaining_tokens.select(\n",
    "            explode(\"Tokens\").alias(\"Key\"),\n",
    "            col(\"InvoiceNo_Items\"),\n",
    "            col(\"Year\")\n",
    "        )\n",
    "\n",
    "        self_joined_data = (\n",
    "            exploded_data\n",
    "            .alias(\"df1\")\n",
    "            .join(exploded_data\n",
    "            .alias(\"df2\"), \"Key\")\n",
    "        )\n",
    "        renamed_data = (\n",
    "            self_joined_data\n",
    "            .select(col(\"df1.Key\").alias(\"Key\"),\n",
    "                     col(\"df1.InvoiceNo_Items\").alias(\"InvoiceNo_Items_1\"),\n",
    "                     col(\"df1.Year\").alias(\"Year_1\"),\n",
    "                     col(\"df2.InvoiceNo_Items\").alias(\"InvoiceNo_Items_2\"),\n",
    "                     col(\"df2.Year\").alias(\"Year_2\"),\n",
    "                     regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "                     regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        splitted_data = (\n",
    "            renamed_data\n",
    "            .select(\"Key\", \n",
    "                     substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "                     \"Year_1\",\n",
    "                     substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "                     \"Year_2\",\n",
    "                     \"InvoiceNo_1\",\n",
    "                     \"InvoiceNo_2\"\n",
    "            )\n",
    "        )\n",
    "        data = splitted_data.filter(col(\"Year_1\") != col(\"Year_2\"))\n",
    "        \n",
    "        def jaccard_similarity(s1, s2):\n",
    "            set1 = set(s1.split())\n",
    "            set2 = set(s2.split())\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return (intersection) / union\n",
    "\n",
    "        jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "        sort_tuple_udf = udf(lambda pair: str(tuple(sorted(eval(pair)))), StringType())\n",
    "        result_df = (\n",
    "            data\n",
    "            .withColumn(\"Similarity\", \n",
    "                        jaccard_similarity_udf(col(\"InvoiceNo_Items_1\"), col(\"InvoiceNo_Items_2\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        result_df = (\n",
    "            result_df\n",
    "            .withColumn(\"pair\", \n",
    "                        concat(lit(\"(\"), col(\"InvoiceNo_1\"), lit(\",\"), col(\"InvoiceNo_2\"), lit(\")\"))\n",
    "            )\n",
    "            .drop(\"InvoiceNo_1\",\"InvoiceNo_2\")\n",
    "            .drop('Key','InvoiceNo_Items_1','Year_1','InvoiceNo_Items_2','Year_2')\n",
    "            .withColumn(\"sorted_pair\", sort_tuple_udf(\"pair\"))\n",
    "            .drop(\"pair\")\n",
    "        )\n",
    "        \n",
    "        result_df = (\n",
    "            result_df\n",
    "            .select('sorted_pair','Similarity')\n",
    "            .dropDuplicates([\"sorted_pair\", \"Similarity\"])\n",
    "            .orderBy('sorted_pair')\n",
    "            .filter(col(\"Similarity\") >= k)\n",
    "        )\n",
    "\n",
    "        result_format_df = (\n",
    "            result_df\n",
    "            .select(concat_ws(\":\", col(\"sorted_pair\"), col(\"Similarity\")).alias('result'))\n",
    "            .orderBy(regexp_extract(\"result\", r\"\\((\\d+),\", 1).cast(\"int\"))\n",
    "        )\n",
    "\n",
    "        exploded_data.show()\n",
    "        #result_format_df.write.format(\"csv\").option(\"header\", \"false\").save(outputpath)\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62baf2",
   "metadata": {},
   "source": [
    "# Efficiency/Similarity Exact Solution - But join or crossjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c613399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from math import floor\n",
    "\n",
    "class project3:           \n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        spark = SparkSession.builder.master(\"local\").appName(\"project3_df\").getOrCreate()\n",
    "        \n",
    "        # Read the input CSV file and give the column names for them\n",
    "        data = spark.read.csv(inputpath).toDF(\"InvoiceNo\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\")\n",
    "        \n",
    "        # Select needed columns\n",
    "        data = data.select(\"InvoiceNo\", \"Description\",\"InvoiceDate\")\n",
    "        \n",
    "        # Extract month and year from the InvoiceDate field \n",
    "        data_filtered = data.withColumn(\"Year\",  split(split(data[\"InvoiceDate\"], ' ')[0], '/')[2])\n",
    "        data_filtered = data_filtered.drop('InvoiceDate')\n",
    "        \n",
    "        # Concatenate the items group by InvoiceNo and year then put them into one column\n",
    "        grouped_data = (\n",
    "                   data_filtered\n",
    "                   .groupBy(\"InvoiceNo\", \"Year\")\n",
    "                   .agg(concat_ws(\" \", collect_list(\"Description\")).alias(\"Description\"))\n",
    "                )\n",
    "        \n",
    "        data_combined_InvoiceNo_Items = (\n",
    "                   grouped_data\n",
    "                   .withColumn(\"InvoiceNo_Items\", concat_ws(\",\", col(\"InvoiceNo\"), col(\"Description\")))   \n",
    "                )\n",
    "        \n",
    "        # Calculate the tokens' frequency and get the sorted token list according to their frequencies\n",
    "        cal_token_frequencies = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .select('InvoiceNo','Description')\n",
    "                    .withColumn(\"tokens\", split(\"Description\", \" \"))\n",
    "                )\n",
    "        df_exploded = (\n",
    "                    cal_token_frequencies\n",
    "                    .select(\"InvoiceNo\", explode(\"tokens\").alias(\"token\"))\n",
    "                )\n",
    "        token_frequencies = (\n",
    "                    df_exploded\n",
    "                    .groupBy(\"token\")\n",
    "                    .agg(count(\"token\").alias(\"frequency\"))\n",
    "                    .orderBy('frequency')\n",
    "                )\n",
    "        tokens = token_frequencies.select(\"token\")\n",
    "        token_list = [row.token for row in tokens.collect()]\n",
    "\n",
    "        # Define a function to calculate the length of prefixes\n",
    "        def calculate_prefix(description, k):\n",
    "            description = set(description.split())\n",
    "            length = floor(len(description) - len(description)*float(k) + 1)\n",
    "            return length\n",
    "        \n",
    "        # udf transform\n",
    "        calculate_prefix_length_udf = udf(calculate_prefix)\n",
    "\n",
    "        df_prefix_length = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .withColumn(\"Prefix_Length\", calculate_prefix_length_udf(col(\"Description\"), lit(k)))\n",
    "                )\n",
    "        \n",
    "        # Define a function to find corresponding prefixes according to their legnths\n",
    "        def find_prefix(description,prefix_length):\n",
    "            prefixes = []\n",
    "            count = 0\n",
    "            for token in token_list:\n",
    "                if count < int(prefix_length):\n",
    "                    if token in description:\n",
    "                        prefixes.append(token)\n",
    "                        count += 1\n",
    "            return prefixes\n",
    "        \n",
    "        # udf transform\n",
    "        find_prefix_udf = udf(find_prefix, ArrayType(StringType()))  \n",
    "        \n",
    "        # Obtain the prefixes using the function above\n",
    "        data_get_prefixes = (\n",
    "                    df_prefix_length\n",
    "                    .withColumn(\"Prefixes\", find_prefix_udf(col(\"Description\"),col('Prefix_Length')))\n",
    "                )\n",
    "        \n",
    "        # Explode the prefixes to get new keys for records\n",
    "        exploded_prefixes = (\n",
    "                    data_get_prefixes\n",
    "                    .select(explode(\"Prefixes\").alias(\"Key\"),col(\"InvoiceNo_Items\"),col(\"Year\"))\n",
    "                    .drop('InvoiceNo')\n",
    "                    .drop('Description')\n",
    "                    .drop('Prefix_Length')\n",
    "                )\n",
    "        \n",
    "        # Self-join the data for calculating the similarity\n",
    "        self_joined_data = (\n",
    "                    exploded_prefixes\n",
    "                    .alias(\"df1\")\n",
    "                    .join(exploded_prefixes\n",
    "                    .alias(\"df2\"), \"Key\")\n",
    "                )\n",
    "        \n",
    "        # Rearrange the data \n",
    "        rearrange_data = (\n",
    "                    self_joined_data\n",
    "                    .select(col(\"df1.Key\").alias(\"Key\"),\n",
    "                             col(\"df1.InvoiceNo_Items\").alias(\"InvoiceNo_Items_1\"),\n",
    "                             col(\"df1.Year\").alias(\"Year_1\"),\n",
    "                             col(\"df2.InvoiceNo_Items\").alias(\"InvoiceNo_Items_2\"),\n",
    "                             col(\"df2.Year\").alias(\"Year_2\"),\n",
    "                             regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "                             regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "                )\n",
    "        )\n",
    "        \n",
    "        # Remove the same year records\n",
    "        filter_year_data = (\n",
    "                    rearrange_data\n",
    "                    .select(\"Key\", \n",
    "                             substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "                             \"Year_1\",\n",
    "                             substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "                             \"Year_2\",\n",
    "                             \"InvoiceNo_1\",\n",
    "                             \"InvoiceNo_2\"\n",
    "                )\n",
    "                    \n",
    "        )\n",
    "        filter_year_data = filter_year_data.filter(col(\"Year_1\") != col(\"Year_2\"))\n",
    "        \n",
    "        # Define a function to calculate the jaccard similarity between two item lists\n",
    "        def calculate_jaccard_similarity(s1, s2):\n",
    "            set1 = set(s1.split())\n",
    "            set2 = set(s2.split())\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return (intersection) / union\n",
    "\n",
    "        # udf transform\n",
    "        jaccard_similarity_udf = udf(calculate_jaccard_similarity, FloatType())\n",
    "        \n",
    "        # # udf transform\n",
    "        sort_pair_udf = udf(lambda pair: str(tuple(sorted(eval(pair)))), StringType())\n",
    "        \n",
    "        # Calculate the similarity between two item columns\n",
    "        similarity_data = (\n",
    "                    filter_year_data\n",
    "                    .withColumn(\"Similarity\", \n",
    "                    jaccard_similarity_udf(col(\"InvoiceNo_Items_1\"), col(\"InvoiceNo_Items_2\"))\n",
    "                )\n",
    "        )\n",
    "        \n",
    "        # Combine two InvoiceNo into a pair and drop redundant columns\n",
    "        similarity_pair_data = (\n",
    "                    similarity_data\n",
    "                    .withColumn(\"pair\", \n",
    "                    concat(lit(\"(\"), col(\"InvoiceNo_1\"), lit(\",\"), col(\"InvoiceNo_2\"), lit(\")\"))\n",
    "                )\n",
    "                    .drop(\"InvoiceNo_1\",\"InvoiceNo_2\")\n",
    "                    .drop('Key','InvoiceNo_Items_1','Year_1','InvoiceNo_Items_2','Year_2')\n",
    "                    .withColumn(\"sorted_pair\", sort_pair_udf(\"pair\"))\n",
    "                    .drop(\"pair\")\n",
    "        )\n",
    "        \n",
    "        # Drop duplicated records and filter records by threshold\n",
    "        similarity_pair_filtered_data = (\n",
    "                    similarity_pair_data\n",
    "                    .select('sorted_pair','Similarity')\n",
    "                    .dropDuplicates([\"sorted_pair\", \"Similarity\"])\n",
    "                    .orderBy('sorted_pair')\n",
    "                    .filter(col(\"Similarity\") >= k)\n",
    "        )\n",
    "        \n",
    "        # Obtain the final result by concatenating pair and similarity with the symbol colon\n",
    "        final_result = (\n",
    "                    similarity_pair_filtered_data\n",
    "                    .select(concat_ws(\":\", col(\"sorted_pair\"), col(\"Similarity\")).alias('result'))\n",
    "                    .orderBy(regexp_extract(\"result\", r\"\\((\\d+),\", 1).cast(\"int\"))\n",
    "        )\n",
    "\n",
    "        # Remove the empty space in the pairs\n",
    "        final_result = final_result.withColumn(\"result\", regexp_replace(\"result\", \",\\\\s*\", \",\"))\n",
    "        \n",
    "        # Write the final result into csv file\n",
    "        final_result.write.format(\"csv\").option(\"header\", \"false\").save(outputpath)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "      "
   ]
  },
  {
   "cell_type": "raw",
   "id": "721a1d70",
   "metadata": {},
   "source": [
    "+-----+---------+\n",
    "|token|frequency|\n",
    "+-----+---------+\n",
    "|    B|        2|\n",
    "|   DD|        2|\n",
    "|    C|        3|\n",
    "|    A|        3|\n",
    "+-----+---------+\n",
    "\n",
    "\n",
    "\n",
    "+---------+-----------+---------------+----+\n",
    "|InvoiceNo|Description|InvoiceNo_Items|Year|\n",
    "+---------+-----------+---------------+----+\n",
    "|        1|      A B C|        1,A B C|2010|\n",
    "|        2|     A C DD|       2,A C DD|2010|\n",
    "|        3|   A B C DD|     3,A B C DD|2011|\n",
    "+---------+-----------+---------------+----+\n",
    "\n",
    "\n",
    "+---------+----+-----------+---------------+-------------+\n",
    "|InvoiceNo|Year|Description|InvoiceNo_Items|Prefix_Length|\n",
    "+---------+----+-----------+---------------+-------------+\n",
    "|        2|2010|     A C DD|       2,A C DD|            2|\n",
    "|        1|2010|      A B C|        1,A B C|            2|\n",
    "|        3|2011|   A B C DD|     3,A B C DD|            3|\n",
    "+---------+----+-----------+---------------+-------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prefix(description,prefix_length):\n",
    "    prefixes = []\n",
    "    for token in token_list:\n",
    "        for i in range(prefix_length):\n",
    "            if token in description:\n",
    "                prefixes.append(token)\n",
    "    return prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8141a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['XMD','G','G','G','PV','VAT','SW','MOU','G']\n",
    "b = ['G','PV','XMD','G','MOU','SW','G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9abe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(a)\n",
    "b = set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe7d5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G', 'MOU', 'PV', 'SW', 'VAT', 'XMD'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283b411a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G', 'MOU', 'PV', 'SW', 'XMD'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee9650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.intersection(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b854c23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.union(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb12b379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.intersection(b)) / len(a.union(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from math import floor\n",
    "\n",
    "class project3:           \n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        spark = SparkSession.builder.master(\"local\").appName(\"project3_df\").getOrCreate()\n",
    "        \n",
    "        # Read the input CSV file and give the column names for them\n",
    "        data = spark.read.csv(inputpath).toDF(\"InvoiceNo\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\")\n",
    "        \n",
    "        # Select needed columns\n",
    "        data = data.select(\"InvoiceNo\", \"Description\",\"InvoiceDate\")\n",
    "        \n",
    "        # Extract month and year from the InvoiceDate field \n",
    "        data_filtered = data.withColumn(\"Year\",  split(split(data[\"InvoiceDate\"], ' ')[0], '/')[2])\n",
    "        data_filtered = data_filtered.drop('InvoiceDate')\n",
    "        \n",
    "        # Concatenate the items group by InvoiceNo and year then put them into one column\n",
    "        grouped_data = (\n",
    "                   data_filtered\n",
    "                   .groupBy(\"InvoiceNo\", \"Year\")\n",
    "                   .agg(concat_ws(\" \", collect_list(\"Description\")).alias(\"Description\"))\n",
    "                )\n",
    "        \n",
    "        data_combined_InvoiceNo_Items = (\n",
    "                   grouped_data\n",
    "                   .withColumn(\"InvoiceNo_Items\", concat_ws(\",\", col(\"InvoiceNo\"), col(\"Description\")))   \n",
    "                )\n",
    "        \n",
    "        # Calculate the tokens' frequency and get the sorted token list according to their frequencies\n",
    "        cal_token_frequencies = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .select('InvoiceNo','Description')\n",
    "                    .withColumn(\"tokens\", split(\"Description\", \" \"))\n",
    "                )\n",
    "        df_exploded = (\n",
    "                    cal_token_frequencies\n",
    "                    .select(\"InvoiceNo\", explode(\"tokens\").alias(\"token\"))\n",
    "                )\n",
    "        token_frequencies = (\n",
    "                    df_exploded\n",
    "                    .groupBy(\"token\")\n",
    "                    .agg(count(\"token\").alias(\"frequency\"))\n",
    "                    .orderBy('frequency')\n",
    "                )\n",
    "        tokens = token_frequencies.select(\"token\")\n",
    "        token_list = [row.token for row in tokens.collect()]\n",
    "\n",
    "        # Define a function to calculate the length of prefixes\n",
    "        def calculate_prefix(description, k):\n",
    "            description = set(description.split())\n",
    "            length = floor(len(description) - len(description)*float(k) + 1)\n",
    "            return length\n",
    "        \n",
    "        # udf transform\n",
    "        calculate_prefix_length_udf = udf(calculate_prefix)\n",
    "\n",
    "        df_prefix_length = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .withColumn(\"Prefix_Length\", calculate_prefix_length_udf(col(\"Description\"), lit(k)))\n",
    "                )\n",
    "        \n",
    "        # Define a function to find corresponding prefixes according to their legnths\n",
    "        def find_prefix(description,prefix_length):\n",
    "            prefixes = []\n",
    "            count = 0\n",
    "            for token in token_list:\n",
    "                if count < int(prefix_length):\n",
    "                    if token in description:\n",
    "                        prefixes.append(token)\n",
    "                        count += 1\n",
    "            return prefixes\n",
    "        \n",
    "        # udf transform\n",
    "        find_prefix_udf = udf(find_prefix, ArrayType(StringType()))  \n",
    "        \n",
    "        # Obtain the prefixes using the function above\n",
    "        data_get_prefixes = (\n",
    "                    df_prefix_length\n",
    "                    .withColumn(\"Prefixes\", find_prefix_udf(col(\"Description\"),col('Prefix_Length')))\n",
    "                )\n",
    "        \n",
    "        # Explode the prefixes to get new keys for records\n",
    "        df1 = (\n",
    "                    data_get_prefixes\n",
    "                    .select(explode(\"Prefixes\").alias(\"Key\"),col(\"InvoiceNo_Items_1\"),col(\"Year_1\"))\n",
    "                    .drop('InvoiceNo')\n",
    "                    .drop('Description')\n",
    "                    .drop('Prefix_Length')\n",
    "        )\n",
    "        \n",
    "        df2 = (\n",
    "                    data_get_prefixes\n",
    "                    .select(explode(\"Prefixes\").alias(\"Key\"),col(\"InvoiceNo_Items_2\"),col(\"Year_2\"))\n",
    "                    .drop('InvoiceNo')\n",
    "                    .drop('Description')\n",
    "                    .drop('Prefix_Length')\n",
    "        )\n",
    "        # Define a function to calculate the jaccard similarity between two item lists\n",
    "        def calculate_jaccard_similarity(s1, s2):\n",
    "            set1 = set(s1.split())\n",
    "            set2 = set(s2.split())\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return float((intersection) / union)\n",
    "\n",
    "        # udf transform\n",
    "        jaccard_similarity_udf = udf(calculate_jaccard_similarity, FloatType())\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Self-join the data for calculating the similarity\n",
    "        self_joined_data = (\n",
    "                    df1\n",
    "                    .crossJoin(df2.select('InvoiceNo_Items_2','Year_2'))\n",
    "                    .select(\n",
    "                           col(\"Key\"),\n",
    "                           col('InvoiceNo_Items_1'),\n",
    "                           col('InvoiceNo_Items_2'),\n",
    "                           col('Year_1'),\n",
    "                           col('Year_2'),\n",
    "                           regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "                           regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "                    )\n",
    "                    .select(\"Key\", \n",
    "                             substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "                             \"Year_1\",\n",
    "                             substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "                             \"Year_2\",\n",
    "                             \"InvoiceNo_1\",\n",
    "                             \"InvoiceNo_2\"\n",
    "                    )\n",
    "                    .filter(col(\"Year_1\") != col(\"Year_2\"))\n",
    "                    .drop('Key','InvoiceNo_Items_1','Year_1','InvoiceNo_Items_2','Year_2')\n",
    "                    .withColumn(\n",
    "                            \"Similarity\", \n",
    "                            jaccard_similarity_udf(col(\"InvoiceNo_Items_1\"), col(\"InvoiceNo_Items_2\"))\n",
    "                        )\n",
    "                    .filter(col('Similarity') >= k)  \n",
    "                    .select(\"InvoiceNo_1\", \"InvoiceNo_2\", \"Similarity\")\n",
    "                    \n",
    "        )\n",
    "        self_joined_data.show()\n",
    "        # # Rearrange the data \n",
    "        # rearrange_data = (\n",
    "        #             self_joined_data\n",
    "        #             .select(col(\"df1.Key\").alias(\"Key\"),\n",
    "        #                      col(\"df1.InvoiceNo_Items\").alias(\"InvoiceNo_Items_1\"),\n",
    "        #                      col(\"df1.Year\").alias(\"Year_1\"),\n",
    "        #                      col(\"df2.InvoiceNo_Items\").alias(\"InvoiceNo_Items_2\"),\n",
    "        #                      col(\"df2.Year\").alias(\"Year_2\"),\n",
    "        #                      regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "        #                      regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "        #         )\n",
    "        # )\n",
    "        \n",
    "        # # Remove the same year records\n",
    "        # filter_year_data = (\n",
    "        #             rearrange_data\n",
    "        #             .select(\"Key\", \n",
    "        #                      substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "        #                      \"Year_1\",\n",
    "        #                      substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "        #                      \"Year_2\",\n",
    "        #                      \"InvoiceNo_1\",\n",
    "        #                      \"InvoiceNo_2\"\n",
    "        #         )\n",
    "                    \n",
    "        # )\n",
    "        # filter_year_data = filter_year_data.filter(col(\"Year_1\") != col(\"Year_2\"))\n",
    "        \n",
    "        # # # udf transform\n",
    "        # sort_pair_udf = udf(lambda pair: str(tuple(sorted(eval(pair)))), StringType())\n",
    "        \n",
    "        # # Calculate the similarity between two item columns\n",
    "        # similarity_data = (\n",
    "        #             filter_year_data\n",
    "        #             .withColumn(\"Similarity\", \n",
    "        #             jaccard_similarity_udf(col(\"InvoiceNo_Items_1\"), col(\"InvoiceNo_Items_2\"))\n",
    "        #         )\n",
    "        # )\n",
    "        \n",
    "        # # Combine two InvoiceNo into a pair and drop redundant columns\n",
    "        # similarity_pair_data = (\n",
    "        #             similarity_data\n",
    "        #             .withColumn(\"pair\", \n",
    "        #             concat(lit(\"(\"), col(\"InvoiceNo_1\"), lit(\",\"), col(\"InvoiceNo_2\"), lit(\")\"))\n",
    "        #         )\n",
    "        #             .drop(\"InvoiceNo_1\",\"InvoiceNo_2\")\n",
    "        #             .drop('Key','InvoiceNo_Items_1','Year_1','InvoiceNo_Items_2','Year_2')\n",
    "        #             .withColumn(\"sorted_pair\", sort_pair_udf(\"pair\"))\n",
    "        #             .drop(\"pair\")\n",
    "        # )\n",
    "        \n",
    "        # # Drop duplicated records and filter records by threshold\n",
    "        # similarity_pair_filtered_data = (\n",
    "        #             similarity_pair_data\n",
    "        #             .select('sorted_pair','Similarity')\n",
    "        #             .dropDuplicates([\"sorted_pair\", \"Similarity\"])\n",
    "        #             .orderBy('sorted_pair')\n",
    "        #             .filter(col(\"Similarity\") >= k)\n",
    "        # )\n",
    "\n",
    "        # # Obtain the final result by concatenating pair and similarity with the symbol colon\n",
    "        # final_result = (\n",
    "        #             similarity_pair_filtered_data\n",
    "        #             .select(concat_ws(\":\", col(\"sorted_pair\"), col(\"Similarity\")).alias('result'))\n",
    "        #             .orderBy(regexp_extract(\"result\", r\"\\((\\d+),\", 1).cast(\"int\"))\n",
    "        # )\n",
    "\n",
    "        # Write the final result into csv file\n",
    "        #final_result.write.format(\"csv\").option(\"header\", \"false\").save(outputpath)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d40ca",
   "metadata": {},
   "source": [
    "# Efficiency/Similarity Exact Solution - But join or crossjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from math import floor\n",
    "\n",
    "class project3:           \n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        spark = SparkSession.builder.master(\"local\").appName(\"project3_df\").getOrCreate()\n",
    "        \n",
    "        # Read the input CSV file and give the column names for them\n",
    "        data = spark.read.csv(inputpath).toDF(\"InvoiceNo\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\")\n",
    "        \n",
    "        # Select needed columns\n",
    "        data = data.select(\"InvoiceNo\", \"Description\",\"InvoiceDate\")\n",
    "        \n",
    "        # Extract month and year from the InvoiceDate field \n",
    "        data_filtered = data.withColumn(\"Year\",  split(split(data[\"InvoiceDate\"], ' ')[0], '/')[2])\n",
    "        data_filtered = data_filtered.drop('InvoiceDate')\n",
    "        \n",
    "        # Concatenate the items group by InvoiceNo and year then put them into one column\n",
    "        grouped_data = (\n",
    "                   data_filtered\n",
    "                   .groupBy(\"InvoiceNo\", \"Year\")\n",
    "                   .agg(concat_ws(\" \", collect_list(\"Description\")).alias(\"Description\"))\n",
    "                )\n",
    "        \n",
    "        data_combined_InvoiceNo_Items = (\n",
    "                   grouped_data\n",
    "                   .withColumn(\"InvoiceNo_Items\", concat_ws(\",\", col(\"InvoiceNo\"), col(\"Description\")))   \n",
    "                )\n",
    "        \n",
    "        # Calculate the tokens' frequency and get the sorted token list according to their frequencies\n",
    "        cal_token_frequencies = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .select('InvoiceNo','Description')\n",
    "                    .withColumn(\"tokens\", split(\"Description\", \" \"))\n",
    "                )\n",
    "        df_exploded = (\n",
    "                    cal_token_frequencies\n",
    "                    .select(\"InvoiceNo\", explode(\"tokens\").alias(\"token\"))\n",
    "                )\n",
    "        token_frequencies = (\n",
    "                    df_exploded\n",
    "                    .groupBy(\"token\")\n",
    "                    .agg(count(\"token\").alias(\"frequency\"))\n",
    "                    .orderBy('frequency')\n",
    "                )\n",
    "        tokens = token_frequencies.select(\"token\")\n",
    "        token_list = [row.token for row in tokens.collect()]\n",
    "\n",
    "        # Define a function to calculate the length of prefixes\n",
    "        def calculate_prefix(description, k):\n",
    "            description = set(description.split())\n",
    "            length = floor(len(description) - len(description)*float(k) + 1)\n",
    "            return length\n",
    "        \n",
    "        # udf transform\n",
    "        calculate_prefix_length_udf = udf(calculate_prefix)\n",
    "\n",
    "        df_prefix_length = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .withColumn(\"Prefix_Length\", calculate_prefix_length_udf(col(\"Description\"), lit(k)))\n",
    "                )\n",
    "        \n",
    "        # Define a function to find corresponding prefixes according to their legnths\n",
    "        def find_prefix(description,prefix_length):\n",
    "            prefixes = []\n",
    "            count = 0\n",
    "            for token in token_list:\n",
    "                if count < int(prefix_length):\n",
    "                    if token in description:\n",
    "                        prefixes.append(token)\n",
    "                        count += 1\n",
    "            return prefixes\n",
    "        \n",
    "        # udf transform\n",
    "        find_prefix_udf = udf(find_prefix, ArrayType(StringType()))  \n",
    "        \n",
    "        # Obtain the prefixes using the function above\n",
    "        data_get_prefixes = (\n",
    "                    df_prefix_length\n",
    "                    .withColumn(\"Prefixes\", find_prefix_udf(col(\"Description\"),col('Prefix_Length')))\n",
    "                )\n",
    "        \n",
    "        # Explode the prefixes to get new keys for records\n",
    "        df1 = (\n",
    "                    data_get_prefixes\n",
    "                    .select(explode(\"Prefixes\").alias(\"Key\"),col(\"InvoiceNo_Items\").alias('InvoiceNo_Items_1'),col(\"Year\").alias('Year_1'))\n",
    "                    .drop('InvoiceNo')\n",
    "                    .drop('Description')\n",
    "                    .drop('Prefix_Length')\n",
    "        )\n",
    "        \n",
    "        df2 = (\n",
    "                    data_get_prefixes\n",
    "                    .select(explode(\"Prefixes\").alias(\"Key\"),col(\"InvoiceNo_Items\").alias('InvoiceNo_Items_2'),col(\"Year\").alias('Year_2'))\n",
    "                    .drop('InvoiceNo')\n",
    "                    .drop('Description')\n",
    "                    .drop('Prefix_Length')\n",
    "        )\n",
    "        # Define a function to calculate the jaccard similarity between two item lists\n",
    "        def calculate_jaccard_similarity(s1, s2):\n",
    "            set1 = set(s1.split())\n",
    "            set2 = set(s2.split())\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return float((intersection) / union)\n",
    "\n",
    "        # udf transform\n",
    "        jaccard_similarity_udf = udf(calculate_jaccard_similarity, FloatType())\n",
    "        \n",
    "        \n",
    "        # Self-join the data for calculating the similarity\n",
    "        self_joined_data = (\n",
    "                    df1\n",
    "                    .crossJoin(df2.select('InvoiceNo_Items_2','Year_2'))\n",
    "                    .select(\n",
    "                           col(\"Key\"),\n",
    "                           col('InvoiceNo_Items_1'),\n",
    "                           col('InvoiceNo_Items_2'),\n",
    "                           col('Year_1'),\n",
    "                           col('Year_2'),\n",
    "                           regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "                           regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "                    )\n",
    "                    .select(\"Key\", \n",
    "                             substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "                             \"Year_1\",\n",
    "                             substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "                             \"Year_2\",\n",
    "                             \"InvoiceNo_1\",\n",
    "                             \"InvoiceNo_2\"\n",
    "                    )\n",
    "                    .filter(col(\"Year_1\") != col(\"Year_2\"))\n",
    "                    .drop('Key','Year_1','Year_2')\n",
    "                    .withColumn(\n",
    "                            \"Similarity\", \n",
    "                            jaccard_similarity_udf(col(\"InvoiceNo_Items_1\"), col(\"InvoiceNo_Items_2\"))\n",
    "                        )\n",
    "                    .drop('InvoiceNo_Items_1','InvoiceNo_Items_2')\n",
    "                    .filter(col('Similarity') >= k)  \n",
    "                    .select(\"InvoiceNo_1\", \"InvoiceNo_2\", \"Similarity\")    \n",
    "        )\n",
    "               \n",
    "        # udf transform\n",
    "        sort_pair_udf = udf(lambda pair: str(tuple(sorted(eval(pair)))), StringType())\n",
    "        \n",
    "        \n",
    "        # Combine two InvoiceNo into a pair and drop redundant columns\n",
    "        similarity_pair_data = (\n",
    "                    self_joined_data\n",
    "                    .withColumn(\"pair\", \n",
    "                    concat(lit(\"(\"), col(\"InvoiceNo_1\"), lit(\",\"), col(\"InvoiceNo_2\"), lit(\")\"))\n",
    "                )\n",
    "                    .drop(\"InvoiceNo_1\",\"InvoiceNo_2\")\n",
    "                    .drop('Key','InvoiceNo_Items_1','Year_1','InvoiceNo_Items_2','Year_2')\n",
    "                    .withColumn(\"sorted_pair\", sort_pair_udf(\"pair\"))\n",
    "                    .drop(\"pair\")\n",
    "        )\n",
    "        \n",
    "        # Drop duplicated records and filter records by threshold\n",
    "        similarity_pair_filtered_data = (\n",
    "                    similarity_pair_data\n",
    "                    .select('sorted_pair','Similarity')\n",
    "                    .dropDuplicates([\"sorted_pair\", \"Similarity\"])\n",
    "                    .orderBy('sorted_pair')\n",
    "                    \n",
    "        )\n",
    "        \n",
    "        # Obtain the final result by concatenating pair and similarity with the symbol colon\n",
    "        final_result = (\n",
    "                    similarity_pair_filtered_data\n",
    "                    .select(concat_ws(\":\", col(\"sorted_pair\"), col(\"Similarity\")).alias('result'))\n",
    "                    .orderBy(regexp_extract(\"result\", r\"\\((\\d+),\", 1).cast(\"int\"))\n",
    "        )\n",
    "\n",
    "        final_result = final_result.withColumn(\"result\", regexp_replace(\"result\", \",\\\\s*\", \",\"))\n",
    "        # Write the final result into csv file\n",
    "        #final_result.show()\n",
    "        #final_result.write.format(\"csv\").option(\"header\", \"false\").save(outputpath)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3129f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "+-----+\n",
    "|token|\n",
    "+-----+\n",
    "|   GU|\n",
    "|  LEW|\n",
    "|   YA|\n",
    "|   EH|\n",
    "|    A|\n",
    "|    T|\n",
    "|  EUI|\n",
    "|   PV|\n",
    "|   YI|\n",
    "|  MNJ|\n",
    "|   HD|\n",
    "|   HO|\n",
    "|  MJB|\n",
    "|  MOU|\n",
    "|  XMD|\n",
    "|    D|\n",
    "|   NX|\n",
    "|   ZH|\n",
    "|   EK|\n",
    "|    I|\n",
    "+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca431ca3",
   "metadata": {},
   "source": [
    "# Final Result - Submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from math import floor\n",
    "\n",
    "class project3:           \n",
    "    def run(self, inputpath, outputpath, k):\n",
    "        spark = SparkSession.builder.master(\"local\").appName(\"project3_df\").getOrCreate()\n",
    "        \n",
    "        # Read the input CSV file and give the column names for them\n",
    "        data = spark.read.csv(inputpath).toDF(\"InvoiceNo\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\")\n",
    "        \n",
    "        # Select needed columns\n",
    "        data = data.select(\"InvoiceNo\", \"Description\",\"InvoiceDate\")\n",
    "        \n",
    "        # Extract month and year from the InvoiceDate field \n",
    "        data_filtered = data.withColumn(\"Year\",  split(split(data[\"InvoiceDate\"], ' ')[0], '/')[2])\n",
    "        data_filtered = data_filtered.drop('InvoiceDate')\n",
    "        \n",
    "        # Concatenate the items group by InvoiceNo and year then put them into one column\n",
    "        grouped_data = (\n",
    "                   data_filtered\n",
    "                   .groupBy(\"InvoiceNo\", \"Year\")\n",
    "                   .agg(concat_ws(\" \", collect_list(\"Description\")).alias(\"Description\"))\n",
    "                )\n",
    "        \n",
    "        data_combined_InvoiceNo_Items = (\n",
    "                   grouped_data\n",
    "                   .withColumn(\"InvoiceNo_Items\", concat_ws(\",\", col(\"InvoiceNo\"), col(\"Description\")))   \n",
    "                )\n",
    "        \n",
    "        # Calculate the tokens' frequency and get the sorted token list according to their frequencies\n",
    "        cal_token_frequencies = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .select('InvoiceNo','Description')\n",
    "                    .withColumn(\"tokens\", split(\"Description\", \" \"))\n",
    "                )\n",
    "        df_exploded = (\n",
    "                    cal_token_frequencies\n",
    "                    .select(\"InvoiceNo\", explode(\"tokens\").alias(\"token\"))\n",
    "                )\n",
    "        token_frequencies = (\n",
    "                    df_exploded\n",
    "                    .groupBy(\"token\")\n",
    "                    .agg(count(\"token\").alias(\"frequency\"))\n",
    "                    .orderBy('frequency')\n",
    "                )\n",
    "        tokens = token_frequencies.select(\"token\")\n",
    "        token_list = [row.token for row in tokens.collect()]\n",
    "\n",
    "        # Define a function to calculate the length of prefixes\n",
    "        def calculate_prefix(description, k):\n",
    "            description = set(description.split())\n",
    "            length = floor(len(description) - len(description)*float(k) + 1)\n",
    "            return length\n",
    "        \n",
    "        # udf transform\n",
    "        calculate_prefix_length_udf = udf(calculate_prefix)\n",
    "        \n",
    "        # Calculate the length of prefixes for each record\n",
    "        df_prefix_length = (\n",
    "                    data_combined_InvoiceNo_Items\n",
    "                    .withColumn(\"Prefix_Length\", calculate_prefix_length_udf(col(\"Description\"), lit(k)))\n",
    "                )\n",
    "        \n",
    "        # Define a function to find corresponding prefixes according to their legnths\n",
    "        def find_prefix(description,prefix_length):\n",
    "            prefixes = []\n",
    "            count = 0\n",
    "            for token in token_list:\n",
    "                if count < int(prefix_length):\n",
    "                    if token in description.split():\n",
    "                        prefixes.append(token)\n",
    "                        count += 1\n",
    "            return prefixes\n",
    "        \n",
    "        # udf transform\n",
    "        find_prefix_udf = udf(find_prefix, ArrayType(StringType()))  \n",
    "        \n",
    "        # Obtain the prefixes using the function above\n",
    "        data_get_prefixes = (\n",
    "                    df_prefix_length\n",
    "                    .withColumn(\"Prefixes\", find_prefix_udf(col(\"Description\"),col('Prefix_Length')))\n",
    "                )\n",
    "        \n",
    "        # Create two dataframes for self-join, each one explodes the prefixes to get new keys for records\n",
    "        df1 = (\n",
    "                    data_get_prefixes\n",
    "                    .select(\n",
    "                        explode(\"Prefixes\").alias(\"Key\"),\n",
    "                        col(\"InvoiceNo_Items\").alias('InvoiceNo_Items_1'),col(\"Year\").alias('Year_1')\n",
    "                    )\n",
    "                    .drop('InvoiceNo')\n",
    "                    .drop('Description')\n",
    "                    .drop('Prefix_Length')\n",
    "        )\n",
    "        \n",
    "        df2 = (\n",
    "                    data_get_prefixes\n",
    "                    .select(\n",
    "                        explode(\"Prefixes\").alias(\"Key\"),\n",
    "                        col(\"InvoiceNo_Items\").alias('InvoiceNo_Items_2'),col(\"Year\").alias('Year_2')\n",
    "                    )\n",
    "                    .drop('InvoiceNo')\n",
    "                    .drop('Description')\n",
    "                    .drop('Prefix_Length')\n",
    "        )\n",
    "        # Define a function to calculate the jaccard similarity between two item lists\n",
    "        def calculate_jaccard_similarity(s1, s2):\n",
    "            set1 = set(s1.split())\n",
    "            set2 = set(s2.split())\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return float((intersection) / union)\n",
    "\n",
    "        # udf transform\n",
    "        jaccard_similarity_udf = udf(calculate_jaccard_similarity, FloatType())\n",
    "        \n",
    "        # Self-join the data for calculating the similarity and reformat the expected dataframe output \n",
    "        self_joined_data = (\n",
    "                    df1\n",
    "                    .join(df2,'Key')\n",
    "                    .select(\n",
    "                           col(\"Key\"),\n",
    "                           col('InvoiceNo_Items_1'),\n",
    "                           col('InvoiceNo_Items_2'),\n",
    "                           col('Year_1'),\n",
    "                           col('Year_2'),\n",
    "                           regexp_extract(\"InvoiceNo_Items_1\", \"\\\\d+\", 0).alias(\"InvoiceNo_1\"),\n",
    "                           regexp_extract(\"InvoiceNo_Items_2\", \"\\\\d+\", 0).alias(\"InvoiceNo_2\")\n",
    "                    )\n",
    "                    .select(\"Key\", \n",
    "                             substring_index(\"InvoiceNo_Items_1\", \",\", -1).alias(\"InvoiceNo_Items_1\"),\n",
    "                             \"Year_1\",\n",
    "                             substring_index(\"InvoiceNo_Items_2\", \",\", -1).alias(\"InvoiceNo_Items_2\"),\n",
    "                             \"Year_2\",\n",
    "                             \"InvoiceNo_1\",\n",
    "                             \"InvoiceNo_2\"\n",
    "                    )\n",
    "                    .filter(col(\"Year_1\") != col(\"Year_2\"))\n",
    "                    .drop('Key','Year_1','Year_2')\n",
    "                    .withColumn(\n",
    "                            \"Similarity\", \n",
    "                            jaccard_similarity_udf(col(\"InvoiceNo_Items_1\"), col(\"InvoiceNo_Items_2\"))\n",
    "                    )\n",
    "                    .drop('InvoiceNo_Items_1','InvoiceNo_Items_2')\n",
    "                    .filter(col('Similarity') >= k)  \n",
    "                    .select(\"InvoiceNo_1\", \"InvoiceNo_2\", \"Similarity\")    \n",
    "        )\n",
    "               \n",
    "        # udf transform\n",
    "        sort_pair_udf = udf(lambda pair: str(tuple(sorted(eval(pair)))), StringType())\n",
    "        \n",
    "        # Combine two InvoiceNo into a pair and drop redundant columns\n",
    "        similarity_pair_data = (\n",
    "                    self_joined_data\n",
    "                    .withColumn(\n",
    "                            \"pair\", \n",
    "                            concat(lit(\"(\"), col(\"InvoiceNo_1\"), lit(\",\"), col(\"InvoiceNo_2\"), lit(\")\"))\n",
    "                    )\n",
    "                    .drop(\"InvoiceNo_1\",\"InvoiceNo_2\")\n",
    "                    .drop('Key','InvoiceNo_Items_1','Year_1','InvoiceNo_Items_2','Year_2')\n",
    "                    .withColumn(\"sorted_pair\", sort_pair_udf(\"pair\"))\n",
    "                    .drop(\"pair\")\n",
    "        )\n",
    "        \n",
    "        # Drop duplicated records and filter records by threshold\n",
    "        similarity_pair_filtered_data = (\n",
    "                    similarity_pair_data\n",
    "                    .select('sorted_pair','Similarity')\n",
    "                    .dropDuplicates([\"sorted_pair\", \"Similarity\"])\n",
    "                    .orderBy('sorted_pair')  \n",
    "        )\n",
    "        \n",
    "        # Obtain the final result by concatenating pair and similarity with the symbol colon\n",
    "        final_result = (\n",
    "                    similarity_pair_filtered_data\n",
    "                    .select(concat_ws(\":\", col(\"sorted_pair\"), col(\"Similarity\")).alias('result'))\n",
    "                    .orderBy(regexp_extract(\"result\", r\"\\((\\d+),\", 1).cast(\"int\"))\n",
    "        )\n",
    "        \n",
    "        # Remove the empty space in the pairs\n",
    "        final_result = final_result.withColumn(\"result\", regexp_replace(\"result\", \",\\\\s*\", \",\"))\n",
    "        \n",
    "        # Write the final result into csv file\n",
    "        final_result.write.format(\"csv\").option(\"header\", \"false\").save(outputpath)\n",
    "        \n",
    "        # testcase 1: rt: 0m10.627s   testcase 2: rt: 0m22.432s\n",
    "if __name__ == \"__main__\":\n",
    "    project3().run(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4324f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
